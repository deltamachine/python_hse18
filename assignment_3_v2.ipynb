{
  "cells": [
    {
      "metadata": {
        "_uuid": "aa14128f61b4e86618b3784c954bc95aa3cf1122"
      },
      "cell_type": "markdown",
      "source": "# Assignment 3 (alternative version)\n\n\nЗадание: построить feed forward NN модель на pytorch для задачи NER из 4 дз. разрешается использовать эмбеддинги. Необходимо побить бейзлайны.\n\nbaseline 1: 0.0604 random labels\n\nbaseline 2: 0.3966 PoS features + logistic regression\n\nbaseline 3: 0.8122 word2vec cbow embedding + baseline 2 + svm"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd2dff7f032ee05ea3fc3dc1d0299865c1238064"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport scipy.sparse as sp\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\nfrom sklearn import model_selection, metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.base import TransformerMixin\nfrom collections import defaultdict\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch.nn as nn\nimport torch as tt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nSEED=1337",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6ebc93d9463e98c6f7e5ef6f03f928e8a41f851"
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('../input/assign3/ner_short.csv', index_col=0)\ndf.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "  next-next-pos next-next-word next-pos ... sentence_idx           word tag\n0           NNS  demonstrators       IN ...          1.0      Thousands   O\n1           VBP           have      NNS ...          1.0             of   O\n2           VBN        marched      VBP ...          1.0  demonstrators   O\n3            IN        through      VBN ...          1.0           have   O\n4           NNP         London       IN ...          1.0        marched   O\n\n[5 rows x 12 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>next-next-pos</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-word</th>\n      <th>pos</th>\n      <th>prev-pos</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-word</th>\n      <th>prev-word</th>\n      <th>sentence_idx</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>IN</td>\n      <td>of</td>\n      <td>NNS</td>\n      <td>__START1__</td>\n      <td>__START2__</td>\n      <td>__START2__</td>\n      <td>__START1__</td>\n      <td>1.0</td>\n      <td>Thousands</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VBP</td>\n      <td>have</td>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>IN</td>\n      <td>NNS</td>\n      <td>__START1__</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>1.0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>VBN</td>\n      <td>marched</td>\n      <td>VBP</td>\n      <td>have</td>\n      <td>NNS</td>\n      <td>IN</td>\n      <td>NNS</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>1.0</td>\n      <td>demonstrators</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>IN</td>\n      <td>through</td>\n      <td>VBN</td>\n      <td>marched</td>\n      <td>VBP</td>\n      <td>NNS</td>\n      <td>IN</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>1.0</td>\n      <td>have</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NNP</td>\n      <td>London</td>\n      <td>IN</td>\n      <td>through</td>\n      <td>VBN</td>\n      <td>VBP</td>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>1.0</td>\n      <td>marched</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "799441260687f2b7b73bf4611f543bf741ca9572"
      },
      "cell_type": "code",
      "source": "# number of sentences\ndf.sentence_idx.max()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "1500.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a76f33508aa79b47ed663a114fc682d9e43cb5a9"
      },
      "cell_type": "code",
      "source": "# class distribution\ndf.tag.value_counts(normalize=True )",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "O        0.852828\nB-geo    0.027604\nB-gpe    0.020935\nB-org    0.020247\nI-per    0.017795\nB-tim    0.016927\nB-per    0.015312\nI-org    0.013937\nI-geo    0.005383\nI-tim    0.004247\nB-art    0.001376\nI-gpe    0.000837\nI-art    0.000748\nB-eve    0.000628\nI-eve    0.000508\nB-nat    0.000449\nI-nat    0.000239\nName: tag, dtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "39179c4284f27cbcd6abe0689187ef6927b7836d"
      },
      "cell_type": "code",
      "source": "# sentence length\ntdf = df.set_index('sentence_idx')\ntdf['length'] = df.groupby('sentence_idx').tag.count()\ndf = tdf.reset_index(drop=False)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d38d52586c8390264f31c329872e9d7501727442"
      },
      "cell_type": "code",
      "source": "# encode categorial variables\n\nle = LabelEncoder()\ndf['pos'] = le.fit_transform(df.pos)\ndf['next-pos'] = le.fit_transform(df['next-pos'])\ndf['next-next-pos'] = le.fit_transform(df['next-next-pos'])\ndf['prev-pos'] = le.fit_transform(df['prev-pos'])\ndf['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b27ab7eadf410e94abe0dd5815667f2923907ba"
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "   sentence_idx  next-next-pos next-next-word   ...             word tag  length\n0           1.0             18  demonstrators   ...        Thousands   O      48\n1           1.0             33           have   ...               of   O      48\n2           1.0             32        marched   ...    demonstrators   O      48\n3           1.0              9        through   ...             have   O      48\n4           1.0             16         London   ...          marched   O      48\n\n[5 rows x 13 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_idx</th>\n      <th>next-next-pos</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-word</th>\n      <th>pos</th>\n      <th>prev-pos</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-word</th>\n      <th>prev-word</th>\n      <th>word</th>\n      <th>tag</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>9</td>\n      <td>of</td>\n      <td>18</td>\n      <td>39</td>\n      <td>40</td>\n      <td>__START2__</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>33</td>\n      <td>have</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>9</td>\n      <td>18</td>\n      <td>39</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>32</td>\n      <td>marched</td>\n      <td>33</td>\n      <td>have</td>\n      <td>18</td>\n      <td>9</td>\n      <td>18</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>9</td>\n      <td>through</td>\n      <td>32</td>\n      <td>marched</td>\n      <td>33</td>\n      <td>18</td>\n      <td>9</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>16</td>\n      <td>London</td>\n      <td>9</td>\n      <td>through</td>\n      <td>32</td>\n      <td>33</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>marched</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6771e55743257958a66e91674871ab08f6728295"
      },
      "cell_type": "code",
      "source": "# splitting\ny = LabelEncoder().fit_transform(df.tag)\n\ndf_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n                                                                      test_size=0.25, random_state=SEED, shuffle=True)\nprint('train', df_train.shape[0])\nprint('test', df_test.shape[0])",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train 50155\ntest 16719\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9fa3b86bb04756de3858a50dc2dce309a8441964"
      },
      "cell_type": "code",
      "source": "# some wrappers to work with word2vec\n\nclass Word2VecWrapper(TransformerMixin):\n    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n        self.window_ = window\n        self.negative_ = negative\n        self.size_ = size\n        self.iter_ = iter\n        self.is_cbow_ = is_cbow\n        self.w2v = None\n        self.random_state = random_state\n        \n    def get_size(self):\n        return self.size_\n\n    def fit(self, X, y=None):\n        \"\"\"\n        X: list of strings\n        \"\"\"\n        sentences_list = [x.split() for x in X]\n        self.w2v = Word2Vec(sentences_list, \n                            window=self.window_,\n                            negative=self.negative_, \n                            size=self.size_, \n                            iter=self.iter_,\n                            sg=not self.is_cbow_, seed=self.random_state)\n\n        return self\n    \n    def has(self, word):\n        return word in self.w2v\n\n    def transform(self, X):\n        \"\"\"\n        X: a list of words\n        \"\"\"\n        if self.w2v is None:\n            raise Exception('model not fitted')\n        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ])",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10e7bf66f46caac116f7e2738742abec4a5b66ef"
      },
      "cell_type": "code",
      "source": "%%time\n# here we exploit that word2vec is an unsupervised learning algorithm\n# so we can train it on the whole dataset (subject to discussion)\n\nsentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n\nw2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\nw2v_cbow.fit(sentences_list)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 34.9 s, sys: 288 ms, total: 35.2 s\nWall time: 19.7 s\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "56a04291f9387c43e300fceac38d86b6e2edba52"
      },
      "cell_type": "markdown",
      "source": "Аналогично тому, как я делала в дз4: попробуем взять POS-теги, добавить к ним колонку 'sentence_idx', прогнать эти признаки через one-hot encoding, добавить W2V-эмбеддинги для самого слова и его ближайших соседей и загнать все это в нейросеть."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "2625c9b1bd9dd20595bc23585c3aaaf79d644d8a"
      },
      "cell_type": "code",
      "source": "embeding = w2v_cbow\nencoder_pos = OneHotEncoder()\n\nX_train = sp.hstack([\n    embeding.transform(df_train.word),\n    embeding.transform(df_train['next-word']),\n    embeding.transform(df_train['prev-word']),\n    encoder_pos.fit_transform(df_train[['sentence_idx', 'pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n])\n\nX_test = sp.hstack([\n    embeding.transform(df_test.word),\n    embeding.transform(df_test['next-word']),\n    embeding.transform(df_test['prev-word']),\n    encoder_pos.transform(df_test[['sentence_idx', 'pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n])\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, stratify=y_train, \n                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ddb3fd9dcbf2c4ecc42f9bb8c3ca80f30e6b13b"
      },
      "cell_type": "markdown",
      "source": "## Training"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a16b2452047e8d92c2e05998ef04b627772827c"
      },
      "cell_type": "code",
      "source": "class FeedforwardNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(FeedforwardNN, self).__init__()    \n        self.fc = nn.Linear(input_dim, hidden_dim) \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n        out = self.fc(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        return out",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef184d66b377648fa3822a9a457c4f5777787d9d"
      },
      "cell_type": "code",
      "source": "class Trainer:\n    def __init__(self, model, train, valid, test, y_train, y_valid, y_test):\n        self.model = model\n        self.train = train.todense()\n        self.test = test.todense()\n        self.valid = valid.todense()\n        \n        self.y_train = y_train\n        self.y_valid = y_valid\n        self.y_test = y_test\n        \n        self.optimizer = tt.optim.Adam(self.model.parameters())\n        self.scheduler = tt.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, verbose=True, cooldown=5)\n        self.criterion = nn.CrossEntropyLoss()\n\n    def nn_train(self, num_epochs=100):\n        train_size = self.train.shape[0]\n        train_batches = int(np.ceil(train_size / batch_size))\n        valid_size = self.valid.shape[0]\n        valid_batches = int(np.ceil(valid_size / batch_size))\n    \n        for epoch in range(num_epochs):\n            indices = np.random.choice(train_size, train_size, replace=False)        \n            epoch_average_loss = 0\n            \n            for j in range(train_batches):\n                batch_idx = indices[j: j + batch_size]\n                batch_x = tt.tensor(self.train[batch_idx], dtype=torch.float)\n                batch_y = tt.from_numpy(self.y_train[batch_idx])\n                self.optimizer.zero_grad()          \n            \n                pred = self.model.forward(batch_x)\n                loss_1 = self.criterion(pred, batch_y.long())\n                loss_1.backward()\n                self.optimizer.step()\n                epoch_average_loss += loss_1.data.detach().item()\n                epoch_average_loss /= train_batches\n            \n            indices = np.random.choice(valid_size, valid_size, replace=False)        \n            epoch_average_loss_2 = 0\n\n            with tt.no_grad():\n                for z in range(valid_batches):\n                    batch_idx = indices[j: j + batch_size]\n                    batch_x = tt.tensor(self.valid[batch_idx], dtype=torch.float)\n                    batch_y = tt.from_numpy(self.y_valid[batch_idx])\n                    pred = self.model.forward(batch_x)\n                    loss = self.criterion(pred, batch_y.long())\n                    epoch_average_loss_2 += loss.data.detach().item()\n                    epoch_average_loss_2 /= train_batches\n\n            print('Epoch [%d/%d], Loss train: %.4f, Loss valid: %.4f' % (epoch+1, num_epochs, epoch_average_loss, epoch_average_loss_2))\n\n            self.scheduler.step(loss_1)\n        \n    def predict(self):\n        test = tt.tensor(self.test, dtype=torch.float)\n        pred = self.model.forward(test)\n        pred = tt.softmax(pred, dim=-1)\n        pred = pred.detach().numpy()\n        y_pred = np.argmax(pred, axis=1)\n        \n        return y_pred",
      "execution_count": 78,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d082004b5daf5c9bc41e7145ec613bd2e7bb6545"
      },
      "cell_type": "code",
      "source": "batch_size = 64\n\nmodel = FeedforwardNN(2606, 128, 17)\ntrainer = Trainer(model, X_train, X_val, X_test, y_train, y_val, y_test)\ntrainer.nn_train()",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch [1/100], Loss train: 0.0011, Loss valid: 0.0026\nEpoch [2/100], Loss train: 0.0022, Loss valid: 0.0024\nEpoch [3/100], Loss train: 0.0016, Loss valid: 0.0028\nEpoch [4/100], Loss train: 0.0015, Loss valid: 0.0026\nEpoch [5/100], Loss train: 0.0015, Loss valid: 0.0040\nEpoch [6/100], Loss train: 0.0012, Loss valid: 0.0015\nEpoch [7/100], Loss train: 0.0011, Loss valid: 0.0017\nEpoch     6: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/100], Loss train: 0.0016, Loss valid: 0.0016\nEpoch [9/100], Loss train: 0.0017, Loss valid: 0.0021\nEpoch [10/100], Loss train: 0.0019, Loss valid: 0.0021\nEpoch [11/100], Loss train: 0.0016, Loss valid: 0.0012\nEpoch [12/100], Loss train: 0.0010, Loss valid: 0.0020\nEpoch [13/100], Loss train: 0.0021, Loss valid: 0.0017\nEpoch [14/100], Loss train: 0.0009, Loss valid: 0.0019\nEpoch [15/100], Loss train: 0.0019, Loss valid: 0.0027\nEpoch [16/100], Loss train: 0.0015, Loss valid: 0.0024\nEpoch [17/100], Loss train: 0.0012, Loss valid: 0.0023\nEpoch [18/100], Loss train: 0.0021, Loss valid: 0.0023\nEpoch [19/100], Loss train: 0.0019, Loss valid: 0.0009\nEpoch [20/100], Loss train: 0.0006, Loss valid: 0.0017\nEpoch [21/100], Loss train: 0.0009, Loss valid: 0.0013\nEpoch [22/100], Loss train: 0.0011, Loss valid: 0.0019\nEpoch [23/100], Loss train: 0.0011, Loss valid: 0.0020\nEpoch [24/100], Loss train: 0.0011, Loss valid: 0.0016\nEpoch [25/100], Loss train: 0.0016, Loss valid: 0.0015\nEpoch [26/100], Loss train: 0.0009, Loss valid: 0.0009\nEpoch    25: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [27/100], Loss train: 0.0018, Loss valid: 0.0014\nEpoch [28/100], Loss train: 0.0014, Loss valid: 0.0018\nEpoch [29/100], Loss train: 0.0013, Loss valid: 0.0016\nEpoch [30/100], Loss train: 0.0021, Loss valid: 0.0013\nEpoch [31/100], Loss train: 0.0010, Loss valid: 0.0018\nEpoch [32/100], Loss train: 0.0011, Loss valid: 0.0009\nEpoch [33/100], Loss train: 0.0014, Loss valid: 0.0016\nEpoch [34/100], Loss train: 0.0011, Loss valid: 0.0012\nEpoch [35/100], Loss train: 0.0019, Loss valid: 0.0019\nEpoch [36/100], Loss train: 0.0016, Loss valid: 0.0027\nEpoch [37/100], Loss train: 0.0014, Loss valid: 0.0016\nEpoch    36: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [38/100], Loss train: 0.0010, Loss valid: 0.0010\nEpoch [39/100], Loss train: 0.0014, Loss valid: 0.0020\nEpoch [40/100], Loss train: 0.0013, Loss valid: 0.0018\nEpoch [41/100], Loss train: 0.0017, Loss valid: 0.0009\nEpoch [42/100], Loss train: 0.0011, Loss valid: 0.0025\nEpoch [43/100], Loss train: 0.0023, Loss valid: 0.0005\nEpoch [44/100], Loss train: 0.0009, Loss valid: 0.0011\nEpoch [45/100], Loss train: 0.0014, Loss valid: 0.0014\nEpoch [46/100], Loss train: 0.0011, Loss valid: 0.0026\nEpoch [47/100], Loss train: 0.0014, Loss valid: 0.0015\nEpoch [48/100], Loss train: 0.0012, Loss valid: 0.0018\nEpoch    47: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [49/100], Loss train: 0.0017, Loss valid: 0.0015\nEpoch [50/100], Loss train: 0.0009, Loss valid: 0.0013\nEpoch [51/100], Loss train: 0.0016, Loss valid: 0.0012\nEpoch [52/100], Loss train: 0.0012, Loss valid: 0.0018\nEpoch [53/100], Loss train: 0.0019, Loss valid: 0.0007\nEpoch [54/100], Loss train: 0.0018, Loss valid: 0.0023\nEpoch [55/100], Loss train: 0.0018, Loss valid: 0.0014\nEpoch [56/100], Loss train: 0.0019, Loss valid: 0.0021\nEpoch [57/100], Loss train: 0.0014, Loss valid: 0.0018\nEpoch [58/100], Loss train: 0.0010, Loss valid: 0.0010\nEpoch [59/100], Loss train: 0.0010, Loss valid: 0.0015\nEpoch    58: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [60/100], Loss train: 0.0018, Loss valid: 0.0020\nEpoch [61/100], Loss train: 0.0016, Loss valid: 0.0009\nEpoch [62/100], Loss train: 0.0017, Loss valid: 0.0020\nEpoch [63/100], Loss train: 0.0019, Loss valid: 0.0013\nEpoch [64/100], Loss train: 0.0020, Loss valid: 0.0012\nEpoch [65/100], Loss train: 0.0010, Loss valid: 0.0013\nEpoch [66/100], Loss train: 0.0022, Loss valid: 0.0017\nEpoch [67/100], Loss train: 0.0014, Loss valid: 0.0019\nEpoch [68/100], Loss train: 0.0023, Loss valid: 0.0013\nEpoch [69/100], Loss train: 0.0016, Loss valid: 0.0019\nEpoch [70/100], Loss train: 0.0021, Loss valid: 0.0021\nEpoch [71/100], Loss train: 0.0018, Loss valid: 0.0014\nEpoch [72/100], Loss train: 0.0015, Loss valid: 0.0014\nEpoch [73/100], Loss train: 0.0015, Loss valid: 0.0010\nEpoch [74/100], Loss train: 0.0013, Loss valid: 0.0017\nEpoch [75/100], Loss train: 0.0015, Loss valid: 0.0016\nEpoch [76/100], Loss train: 0.0014, Loss valid: 0.0006\nEpoch [77/100], Loss train: 0.0011, Loss valid: 0.0011\nEpoch [78/100], Loss train: 0.0014, Loss valid: 0.0013\nEpoch [79/100], Loss train: 0.0007, Loss valid: 0.0016\nEpoch [80/100], Loss train: 0.0014, Loss valid: 0.0018\nEpoch [81/100], Loss train: 0.0011, Loss valid: 0.0016\nEpoch [82/100], Loss train: 0.0020, Loss valid: 0.0016\nEpoch [83/100], Loss train: 0.0010, Loss valid: 0.0017\nEpoch [84/100], Loss train: 0.0021, Loss valid: 0.0012\nEpoch [85/100], Loss train: 0.0016, Loss valid: 0.0020\nEpoch [86/100], Loss train: 0.0010, Loss valid: 0.0013\nEpoch [87/100], Loss train: 0.0009, Loss valid: 0.0017\nEpoch [88/100], Loss train: 0.0018, Loss valid: 0.0006\nEpoch [89/100], Loss train: 0.0008, Loss valid: 0.0023\nEpoch [90/100], Loss train: 0.0010, Loss valid: 0.0020\nEpoch [91/100], Loss train: 0.0013, Loss valid: 0.0020\nEpoch [92/100], Loss train: 0.0013, Loss valid: 0.0008\nEpoch [93/100], Loss train: 0.0011, Loss valid: 0.0018\nEpoch [94/100], Loss train: 0.0006, Loss valid: 0.0018\nEpoch [95/100], Loss train: 0.0004, Loss valid: 0.0011\nEpoch [96/100], Loss train: 0.0019, Loss valid: 0.0018\nEpoch [97/100], Loss train: 0.0011, Loss valid: 0.0010\nEpoch [98/100], Loss train: 0.0012, Loss valid: 0.0011\nEpoch [99/100], Loss train: 0.0016, Loss valid: 0.0013\nEpoch [100/100], Loss train: 0.0015, Loss valid: 0.0010\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "3d89fdfc8c8e000913ba663f68117d02f7dc1921"
      },
      "cell_type": "markdown",
      "source": "## Evaluation"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2afe2b68408813595ed831a8b26c3f208c9ce7da"
      },
      "cell_type": "code",
      "source": "y_pred = trainer.predict()\nmetrics.f1_score(y_test, y_pred, average='macro')",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 82,
          "data": {
            "text/plain": "0.131548871147835"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "079d6f1578bcceff580e949ca99da36ecf394be2"
      },
      "cell_type": "markdown",
      "source": "Побился только один бейзлайн :("
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}